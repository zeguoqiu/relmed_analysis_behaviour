# PILT task Turing.jl models

"""
    single_p_QL(; N::Int64, n_blocks::Int64, block::Vector{Int64}, valence::AbstractVector, 
                choice, outcomes::Matrix{Float64}, initV::Matrix{Float64}, 
                σ_ρ::Float64=1.0, σ_a::Float64=0.5)

Performs Q-learning for a single participant in a reinforcement learning task, with trial-wise updates of Q-values based on choices and outcomes.

# Arguments
- `N::Int64`: Total number of trials.
- `n_blocks::Int64`: Number of blocks in the task.
- `block::Vector{Int64}`: A vector indicating the block number for each trial.
- `valence::AbstractVector`: A vector of valence values associated with each block, modulating the Q-value updates.
- `choice`: A binary vector representing the participant's choices (e.g., `true` for choosing stimulus A). Not typed to allow for both empirical and simulated data.
- `outcomes::Matrix{Float64}`: A matrix of outcomes for the options, where the first column corresponds to the suboptimal option and the second column to the optimal option.
- `initV::Matrix{Float64}`: Initial Q-values for the options, used as a starting point for learning.
- `σ_ρ::Float64=1.0`: Standard deviation for the prior distribution of the reward sensitivity parameter `ρ`.
- `σ_a::Float64=0.5`: Standard deviation for the prior distribution of the learning rate parameter `a`.

# Returns
- `Qs`: A matrix of updated Q-values for each trial, reflecting the participant's learning over time.

# Details
- The function models the Q-learning process where the Q-values (expected values) for each option are updated based on prediction errors.
- `ρ` is the reward sensitivity parameter, drawn from a truncated normal distribution, which scales the outcomes.
- `a` is the learning rate parameter, drawn from a normal distribution, transformed via a logistic function to compute the learning rate `α`.
- The Q-values are initialized using the provided `initV` matrix and are scaled by `ρ` and modulated by the `valence` of the block in which the trial occurs.
- On each trial, the participant's choice is modeled using a Bernoulli distribution, where the probability is determined by the difference in Q-values of the two options.
- The chosen option's Q-value is updated based on the prediction error (`PE`), which is the difference between the observed outcome and the expected value.
"""
@model function single_p_QL(;
	N::Int64, # Total number of trials
	n_blocks::Int64, # Number of blocks
	block::Vector{Int64}, # Block number
	choice, # Binary choice, coded true for stimulus A. Not typed so that it can be simulated
	outcomes::Matrix{Float64}, # Outcomes for options, second column optimal
	initV::Matrix{Float64}, # Initial Q values,
	prior_ρ::Distribution = truncated(Normal(0., 2.), lower = 0.),
	prior_a::Distribution = Normal(0., 1)
)

	# Priors on parameters
	ρ ~ prior_ρ
	a ~ prior_a

	# Compute learning rate
	α = a2α(a) # hBayesDM uses Phi_approx from Stan. Here, logistic with the variance of the logistic multiplying a to equate the scales to that of a probit function.

	# Initialize Q values, with sign depending on block valence
	Qs = repeat(initV .* ρ, length(block)) .* sign.(outcomes[:, 1])

	# Loop over trials, updating Q values and incrementing log-density
	for i in 1:N
		
		# Define choice distribution
		choice[i] ~ BernoulliLogit(Qs[i, 2] - Qs[i, 1])

		choice_idx::Int64 = choice[i] + 1

		# Prediction error
		PE[i] = outcomes[i, choice_idx] * ρ - Qs[i, choice_idx]

		# Update Q value
		if (i != N) && (block[i] == block[i+1])
			Qs[i + 1, choice_idx] = Qs[i, choice_idx] + α * PE
			Qs[i + 1, 3 - choice_idx] = Qs[i, 3 - choice_idx]
		end
	end

	return Qs, PE

end

"""
    single_p_recip_QL(; N::Int64, n_blocks::Int64, block::Vector{Int64}, valence::AbstractVector, 
                      choice, outcomes::Matrix{Float64}, initV::Matrix{Float64}, 
                      σ_ρ::Float64=1.0, σ_a::Float64=0.5)

Performs a variant of Q-learning for a single participant in a reinforcement learning task, where Q-values are updated reciprocally for the chosen and unchosen options based on choices and outcomes.

# Arguments
- `N::Int64`: Total number of trials.
- `n_blocks::Int64`: Number of blocks in the task.
- `block::Vector{Int64}`: A vector indicating the block number for each trial.
- `valence::AbstractVector`: A vector of valence values associated with each block, modulating the Q-value updates.
- `choice`: A binary vector representing the participant's choices (e.g., `true` for choosing stimulus A). Not typed to allow for both empirical and simulated data.
- `outcomes::Matrix{Float64}`: A matrix of outcomes for the options, where the first column corresponds to the suboptimal option and the second column to the optimal option.
- `initV::Matrix{Float64}`: Initial Q-values for the options, used as a starting point for learning.
- `σ_ρ::Float64=1.0`: Standard deviation for the prior distribution of the reward sensitivity parameter `ρ`.
- `σ_a::Float64=0.5`: Standard deviation for the prior distribution of the learning rate parameter `a`.

# Returns
- `Qs`: A matrix of updated Q-values for each trial, reflecting the participant's learning over time with reciprocal updates.

# Details
- The function models a modified Q-learning process where the Q-values (expected values) for both the chosen and unchosen options are updated based on prediction errors.
- `ρ` is the reward sensitivity parameter, drawn from a truncated normal distribution, which scales the outcomes.
- `a` is the learning rate parameter, drawn from a normal distribution, transformed via a logistic function to compute the learning rate `α`.
- The Q-values are initialized using the provided `initV` matrix and are scaled by `ρ` and modulated by the `valence` of the block in which the trial occurs.
- On each trial, the participant's choice is modeled using a Bernoulli distribution, where the probability is determined by the difference in Q-values of the two options.
- The chosen option's Q-value is updated based on the prediction error (`PE`), which is the difference between the observed outcome and the expected value.
- In this variant, the unchosen option's Q-value is reciprocally updated by subtracting the same `α * PE`, leading to a balance between the two options' values over time.
"""
@model function single_p_recip_QL(;
	N::Int64, # Total number of trials
	n_blocks::Int64, # Number of blocks
	block::Vector{Int64}, # Block number
	valence::AbstractVector, # Valence of each block
	choice, # Binary choice, coded true for stimulus A. Not typed so that it can be simulated
	outcomes::Matrix{Float64}, # Outcomes for options, second column optimal
	initV::Matrix{Float64}, # Initial Q values,
	σ_ρ::Float64 = 5.,
	σ_a::Float64 = 1.
)

	# Priors on parameters
	ρ ~ truncated(Normal(0., σ_ρ), lower = 0.)
	a ~ Normal(0., σ_a)

	# Compute learning rate
	α = a2α(a) # hBayesDM uses Phi_approx from Stan. Here, logistic with the variance of the logistic multiplying a to equate the scales to that of a probit function.

	# Initialize Q values
	Qs = repeat(initV .* ρ, length(block)) .* valence[block]

	# Loop over trials, updating Q values and incrementing log-density
	for i in 1:N
		
		# Define choice distribution
		choice[i] ~ BernoulliLogit(Qs[i, 2] - Qs[i, 1])

		choice_idx::Int64 = choice[i] + 1

		# Prediction error
		PE = outcomes[i, choice_idx] * ρ - Qs[i, choice_idx]

		# Update Q value
		if (i != N) && (block[i] == block[i+1])
			Qs[i + 1, choice_idx] = Qs[i, choice_idx] + α * PE
			Qs[i + 1, 3 - choice_idx] = Qs[i, 3 - choice_idx] - α * PE
		end
	end

	return Qs

end